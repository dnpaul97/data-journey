# Introduction

Welcome to the `bigdata` repository! This repository is designed as a learning platform for various big data technologies and practices. The primary goal is to explore and test different concepts and tools in the realm of big data, with a keen focus on Infrastructure as Code (IaaC) to deploy services on Google Cloud Platform (GCP), including but not limited to Google Cloud Storage (GCS) and Google Kubernetes Engine (GKE).

## Key Features

- **IaaC on GCP**: Utilize Infrastructure as Code concepts to seamlessly deploy services such as GCS and GKE on GCP, enabling scalable and efficient management of cloud resources.

- **Python Modules and Packages**: Incorporate Python scripts as modules in packages to demonstrate best practices in Python programming and modular design.

- **Package Managers**: Experiment with new package managers like Pipfile to manage dependencies more effectively and securely.

- **Containerization with Docker**: Learn how to containerize solutions using Dockerfiles and test them with Docker Compose, facilitating consistent development environments and streamlining deployment processes.

- **Workflow and Pipelines**: Implement workflow tests for continuous integration and delivery pipelines using tools like Argo or Prefect, leveraging a native Kubernetes (k8s) approach for automated and scalable workflows.

- **Horizontal Scaling with Dask**: Explore horizontal scaling solutions using Dask to efficiently process large datasets and perform complex computations across multiple nodes.

This repository serves as a sandbox for testing, learning, and experiencing the vast possibilities within the realm of big data and cloud technologies. Whether you're new to these concepts or looking to expand your knowledge, this repo offers a practical approach to understanding and applying big data solutions.